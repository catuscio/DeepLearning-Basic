# 로지스틱 회귀(Logistic Regression)
로지스틱 회귀란 참(1)과 거짓(0) 사이를 구분하는 S자 형태의 선을 그어주는 작업을 의미한다.

## 시그모이드 함수(Sigmoid Function)
S자 형태로 그려지는 함수를 시그모이드 함수라고 한다. 함수 원형은 다음과 같다.\
$$y = \frac{1}{1 + e^{-(ax+b)}}$$
\
시그모이드 함수에서도, 선형회귀와 마찬가지로 $a$와 $b$값을 구해야 한다.
|값|의미|
|-|-|
|a|곡선의 경사도. 클 수록 가파르다.|
|b|곡선의 좌우 이동. 클 수록 왼쪽으로 치우친다.|

- $a$값이 작아지면 오차가 무한대로 커진다.
- $b$값이 너무 크거나 작으면 오차가 커진다.(적정값 찾아야 한다)

## $a$와 $b$값 구하기
두 값을 구하기 위해서는 경사 하강법을 이용해야 한다.\
경사 하강법은 먼저 오차를 구한 뒤 오차가 최소가되는 지점을 탐색하는 방식이므로, 먼저 시그모이드 함수의 오차 공식을 도출해야 한다.

### 로그 함수
시그모이드 함수의 치역은 0과 1 둘 중 하나이다.\
예측값을 $h$라 할 때, 실제값에 따라 오차는 다음 두 로그함수의 양상으로 나타난다.\
- __실제값이 1일 경우__
$$오차 = -\log h$$
- __실제값이 0일 경우__
$$오차 = -\log{(1-h)}$$
\
따라서 실제값을 $y$라고 할 때, 오차 공식은 다음과 같다.\
$$오차 = -((y * log h) + ((1 - y) * log (1-h)))$$


---
## 코딩으로 해보자
https://github.com/catuscio/DeepLearning-Basic/blob/main/colab%20workout/Logistic_regression.ipynb
