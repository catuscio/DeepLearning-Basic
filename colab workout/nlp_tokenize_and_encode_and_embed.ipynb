{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0xd+9bUmh1TzV1SaLFwc4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/catuscio/DeepLearning-Basic/blob/main/nlp_tokenize_and_encode_and_embed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# __텐서플로우 케라스를 활용한 자연어 처리 기초__\n"
      ],
      "metadata": {
        "id": "coPoKUSHgryY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 단어를 토큰화하고 원-핫 인코딩 적용하기\n",
        "## tokenize and one-hot encoding"
      ],
      "metadata": {
        "id": "xvuvWqiCkO98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 문장 토큰화하기"
      ],
      "metadata": {
        "id": "keMXDLgSik_D"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "text = \"오랫동안 꿈꾸는 이는 그 꿈을 닮아간다\"\n",
        "\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts([text])    #토큰화하기 위해 .fit_on_texts()에 입력\n",
        "\n",
        "print(token.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4vhYPtLhIBL",
        "outputId": "13465b18-d289-4d9b-a791-4e58a40cfe1c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'오랫동안': 1, '꿈꾸는': 2, '이는': 3, '그': 4, '꿈을': 5, '닮아간다': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 토큰의 인덱스로만 채워진 새로운 배열 생성"
      ],
      "metadata": {
        "id": "LCuBft4_h6Vy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = token.texts_to_sequences([text])\n",
        "\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmfidecGizcS",
        "outputId": "322177b4-078e-42d6-9a0e-528a1a4fa2fa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 3, 4, 5, 6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 원-핫 인코딩 함수 to_categorical() 사용"
      ],
      "metadata": {
        "id": "rF2tIin3i5X9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "#인덱스 수에 하나를 추가해서 원-핫 인코딩 배열 생성\n",
        "word_size = len(token.word_index) + 1   #맨 앞 주소는 인덱스이므로 0으로 비워두어야 하기 때문\n",
        "x = to_categorical(x, num_classes=word_size)  #x를 입력값으로 하고 분류의 개수(데이터 개수)를 word_size로 한다다\n",
        "\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17VRalc5jT4q",
        "outputId": "877b1bc7-be1b-4e5c-f643-52bae51e573d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 단어 임베딩하기\n",
        "## word embedding"
      ],
      "metadata": {
        "id": "2_gNEMVCjigQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어 토큰으로 이루어진 거대한 말뭉치를 원-핫 인코딩으로 벡터화하면, 매우 많은 단어벡터(배열)를 생성해야만 한다.\\\n",
        "이는 원-핫 인코딩의 벡터가 희소벡터이기 때문이다.\\\n",
        "단어 임베딩을 통해 실수형 밀집벡터로 변환하는 것으로, 이러한 공간적 낭비를 해소할 수 있다."
      ],
      "metadata": {
        "id": "YczxOUBskIdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(16, 4)) ##입력될 총 단어의 수는 16개, 임베딩 후 출력되는 벡터 크기는 4."
      ],
      "metadata": {
        "id": "1gNMWo11mHVr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding(samples, sequence_length, input_length) 함수를 통해 임베딩 공간을 생성한 모습이다.\\\n",
        "임베딩 공간에서는 단어 간의 유사도를 계산하여 원-핫 인코딩의 각 벡터를 새로운 수치로 변환한다. 해당 과정에서는 오차 역전파가 사용된다.\\\n",
        "함수의 매개변수는 사용할 단어의 개수(samples), 한번에 사용할 단어의 개수(input_length)로 설정한다."
      ],
      "metadata": {
        "id": "ESPmtPmpmPOL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9i2Svqhqpfkg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
